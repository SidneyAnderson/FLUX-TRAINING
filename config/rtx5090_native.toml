# RTX 5090 Native FLUX Training Configuration
# CUDA 13.0 + Blackwell (sm_120) - Zero Compromises
# Optimized for 99.9% face accuracy and maximum performance

[model_arguments]
# FLUX model paths - adjust these to your model locations
pretrained_model_name_or_path = "./models/flux1-dev.safetensors"
clip_l = "./models/clip_l.safetensors"
t5xxl = "./models/t5xxl_fp16.safetensors"
ae = "./models/ae.safetensors"

[network_arguments]
# Network configuration for LoRA
network_module = "networks.lora_flux"
network_dim = 128                # Maximum quality (higher = better, but slower)
network_alpha = 64               # Optimal for 128 dim (usually dim/2)
network_train_unet_only = true   # Only train UNet, not text encoders
network_dropout = 0.05           # Light regularization to prevent overfitting

[optimizer_arguments]
# Prodigy optimizer - adaptive learning rate
optimizer_type = "prodigyopt"
optimizer_args = [
    "decouple=True",             # Decouple weight decay
    "weight_decay=0.01",         # L2 regularization
    "betas=[0.9,0.999]",         # Adam beta parameters
    "eps=1e-8",                  # Numerical stability
    "d_coef=2.0",                # Prodigy D coefficient
    "growth_rate=1.02",          # Learning rate growth
    "use_bias_correction=True",  # Bias correction for better convergence
    "safeguard_warmup=True"      # Warmup to prevent early instability
]
learning_rate = 1.0              # Prodigy handles actual LR adaptively
lr_scheduler = "constant"        # Constant with Prodigy's adaptive rate

[training_arguments]
# Training parameters
max_train_steps = 1500           # Total training steps
train_batch_size = 1             # Batch size per GPU
gradient_checkpointing = true    # Save memory by recomputing activations
gradient_accumulation_steps = 4  # Effective batch size = 1 * 4 = 4

# Mixed precision training
mixed_precision = "bf16"         # BF16 for Blackwell tensor cores
full_bf16 = true                 # Full BF16 computation (not just weights)

# FLUX-specific parameters
timestep_sampling = "shift"      # Shifted timestep sampling
discrete_flow_shift = 3.1582     # Optimal shift value for FLUX
model_prediction_type = "raw"    # Raw prediction (not v-prediction)
guidance_scale = 3.5             # Classifier-free guidance scale
loss_type = "l2"                 # L2 loss (MSE)

# RTX 5090 Native Optimizations
mem_eff_attn = false             # Disable (we use xformers)
xformers = true                  # Use our compiled xformers with sm_120
sdpa = false                     # Disable SDPA (xformers is better)

# Caching for 32GB VRAM
cache_latents = true             # Cache VAE latents
cache_latents_to_disk = false    # Keep in VRAM (we have 32GB)
cache_text_encoder_outputs = true             # Cache text encoder outputs
cache_text_encoder_outputs_to_disk = false    # Keep in VRAM

# Advanced Blackwell features
enable_cudnn_benchmark = true    # CuDNN autotuning
cudnn_deterministic = false      # Allow non-deterministic for speed
use_tensorcore = true            # Force tensor core usage
tf32_mode = false                # Disable TF32 (BF16 is better on Blackwell)

[dataset_arguments]
# Dataset configuration
train_data_dir = "./dataset"     # Your training images directory
resolution = "1024,1024"         # Training resolution
enable_bucket = false            # Disable bucketing for exact resolution
cache_info_to_disk = false       # Keep metadata in memory
shuffle_caption = false          # Don't shuffle captions
keep_tokens = 1                  # Keep first token (trigger word)
caption_extension = ".txt"       # Caption file extension

# Data augmentation (optional)
# flip_aug = false               # No horizontal flip
# color_aug = false              # No color augmentation

[saving_arguments]
# Model saving configuration
save_every_n_steps = 100         # Save checkpoint every 100 steps
save_state = true                # Save optimizer state for resume
save_model_as = "safetensors"    # Use safetensors format
save_precision = "bf16"          # Save in BF16 to reduce size
output_dir = "./output"          # Output directory
output_name = "rtx5090_native"   # Output file prefix

# Keep limited checkpoints to save space
# save_last_n_models = 3         # Only keep last 3 checkpoints

[logging_arguments]
# Logging configuration
logging_dir = "./logs"           # TensorBoard logs
log_with = "tensorboard"         # Use TensorBoard
log_every_n_steps = 10           # Log every 10 steps

# Additional logging (optional)
# wandb_project = "flux-training" # W&B project name (if using wandb)
# wandb_run_name = "rtx5090"      # W&B run name

[sample_prompt_arguments]
# Sample generation during training
sample_every_n_steps = 50        # Generate sample every 50 steps
sample_sampler = "euler"         # Euler sampler for generation
sample_prompts = [
    # Replace 't4r4woman' with your trigger word
    "t4r4woman portrait, high quality --w 1024 --h 1024 --d 1 --l 3.5 --s 20",
    "t4r4woman smiling, professional photo --w 1024 --h 1024 --d 1 --l 3.5 --s 20",
    "t4r4woman close-up face --w 1024 --h 1024 --d 1 --l 3.5 --s 20"
]

[performance]
# RTX 5090 specific performance settings
dataloader_num_workers = 8       # Parallel data loading
persistent_data_loader_workers = true  # Keep workers alive

# PyTorch compilation (experimental, may increase speed)
# torch_compile = true
# torch_compile_backend = "inductor"
# torch_compile_mode = "max-autotune"

# Note: Uncomment torch_compile options above if you want to try
# PyTorch 2.0 compilation. It may provide 10-20% speedup but
# increases initial compilation time.

################################################################################
# CONFIGURATION NOTES
################################################################################
#
# 1. TRIGGER WORD: Replace 't4r4woman' with your actual trigger word
#
# 2. DATASET STRUCTURE:
#    dataset/
#    ├── image1.jpg
#    ├── image1.txt  (contains: "t4r4woman")
#    ├── image2.jpg
#    ├── image2.txt  (contains: "t4r4woman")
#    └── ...
#
# 3. MODELS: Download from HuggingFace and place in ./models/
#    - flux1-dev.safetensors
#    - clip_l.safetensors
#    - t5xxl_fp16.safetensors (use FP16 version to save VRAM)
#    - ae.safetensors
#
# 4. TRAINING TIME: ~1-1.5 hours for 1500 steps on RTX 5090
#
# 5. MEMORY USAGE: ~28-30GB VRAM with these settings
#
# 6. ADJUSTMENTS:
#    - If OOM: Reduce network_dim to 64 or enable cache_to_disk
#    - If too slow: Reduce max_train_steps to 1000
#    - If quality low: Increase network_dim to 256 (requires more VRAM)
#
################################################################################
